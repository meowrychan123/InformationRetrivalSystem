{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Libraries and NLTK Setups**"
      ],
      "metadata": {
        "id": "AQIBRbCB8DfD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omBa-ps8e3VL",
        "outputId": "d61f3b4f-34fb-4098-ff2c-504132bf12f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk . download ('stopwords')\n",
        "nltk . download ('punkt')\n",
        "nltk . download ('wordnet')\n",
        "import os\n",
        "import string\n",
        "import logging\n",
        "import re\n",
        "from collections import defaultdict , Counter\n",
        "from nltk . corpus import stopwords\n",
        "from nltk . tokenize import word_tokenize\n",
        "from nltk . stem import WordNetLemmatizer\n",
        "\n",
        "STOPWORDS = set( stopwords . words ('english') )\n",
        "LEMMATIZER = WordNetLemmatizer ()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading Text Files**\n",
        "\n",
        "Reading all .txt files in a folder and returning a dictionary with filenames as keys and content as values."
      ],
      "metadata": {
        "id": "fXOKNJI18bpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_text_files(folder_path):\n",
        "    data = {}\n",
        "    doc_id_to_filename = {}\n",
        "    doc_id = 0\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
        "                data[doc_id] = file.read()\n",
        "                doc_id_to_filename[doc_id] = filename\n",
        "                logging.info(f\"Loaded file: {filename} with doc_id: {doc_id}\")\n",
        "                doc_id += 1\n",
        "    return data, doc_id_to_filename\n"
      ],
      "metadata": {
        "id": "Sm3V3ahVf171"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Cleaning Process**\n",
        "Performing text cleaning i.e., removing special characters, tokenization, stop word removal, and lemmatization."
      ],
      "metadata": {
        "id": "mmQcjab083Lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Remove special characters\n",
        "    tokens = word_tokenize(text)\n",
        "    cleaned_tokens = [LEMMATIZER.lemmatize(word) for word in tokens if word not in STOPWORDS]\n",
        "    return cleaned_tokens\n"
      ],
      "metadata": {
        "id": "oOSvHqdggpVw"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building Inverted Index**\n",
        "Building an inverted index from the cleaned text data and tracking the term frequencies."
      ],
      "metadata": {
        "id": "22Dq3qob9hAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_inverted_index(data):\n",
        "    inverted_index = defaultdict(set)\n",
        "    term_frequencies = Counter()  # Track term frequencies\n",
        "    for doc_id, content in data.items():\n",
        "        cleaned_tokens = clean_text(content)\n",
        "        for token in cleaned_tokens:\n",
        "            inverted_index[token].add(doc_id)\n",
        "            term_frequencies[token] += 1\n",
        "    return inverted_index, term_frequencies"
      ],
      "metadata": {
        "id": "tE5yjLQHhMqi"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Boolean Queries: AND Operation**\n",
        "\n",
        "Performing an AND query on the terms using the inverted index."
      ],
      "metadata": {
        "id": "e08ouo_m98bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def boolean_and(terms, inverted_index):\n",
        "    result_set = inverted_index.get(terms[0], set())\n",
        "    for term in terms[1:]:\n",
        "        result_set = result_set.intersection(inverted_index.get(term, set()))\n",
        "    return result_set"
      ],
      "metadata": {
        "id": "NaiGq9ujhY1n"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **OR Operation**\n",
        "Performing an OR query on the terms using the inverted index."
      ],
      "metadata": {
        "id": "gR0en8AF-OWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def boolean_or(terms, inverted_index):\n",
        "    result_set = set()\n",
        "    for term in terms:\n",
        "        result_set = result_set.union(inverted_index.get(term, set()))\n",
        "    return result_set"
      ],
      "metadata": {
        "id": "gnk5OqE_heUN"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NOT Operation**\n",
        "Performing a NOT query on the term using the inverted index."
      ],
      "metadata": {
        "id": "xEOXzmke-sev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def boolean_not(term, inverted_index, total_docs):\n",
        "    return set(range(total_docs)) - inverted_index.get(term, set())"
      ],
      "metadata": {
        "id": "xadXdrkVhiGF"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Processing Boolean Queries**\n",
        "\n",
        "Processing boolean queries with AND, OR, and NOT operations."
      ],
      "metadata": {
        "id": "qLbgVMaf_-mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def boolean_query(query, inverted_index, total_docs):\n",
        "    tokens = query.lower().split()\n",
        "    if 'and' in tokens:\n",
        "        terms = [term for term in tokens if term not in ['and', 'or', 'not']]\n",
        "        return boolean_and(terms, inverted_index)\n",
        "    elif 'or' in tokens:\n",
        "        terms = [term for term in tokens if term not in ['and', 'or', 'not']]\n",
        "        return boolean_or(terms, inverted_index)\n",
        "    elif 'not' in tokens:\n",
        "        return boolean_not(tokens[1], inverted_index, total_docs)\n",
        "    else:\n",
        "        return inverted_index.get(tokens[0], set())"
      ],
      "metadata": {
        "id": "xP_D1YF6hkmF"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Converting doc_ids to Filenames**\n",
        "Converting doc_ids back to the original filenames."
      ],
      "metadata": {
        "id": "jfIeywC4APlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_doc_ids_to_filenames(result_set, doc_id_to_filename):\n",
        "    return [doc_id_to_filename[doc_id] for doc_id in result_set if doc_id in doc_id_to_filename]"
      ],
      "metadata": {
        "id": "5MKrMhpihnff"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Queries Result Files**\n",
        "Writing the results of queries to a new txt file in a sperate folder."
      ],
      "metadata": {
        "id": "LjkOtRFLAy9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def write_query_results(queries, inverted_index, doc_id_to_filename, total_docs):\n",
        "    folder_path = \"/content/drive/MyDrive/Resultfrom_queries\"\n",
        "    os.makedirs(folder_path, exist_ok=True)\n",
        "    results_file_path = os.path.join(folder_path, \"query_results.txt\")\n",
        "\n",
        "    with open(results_file_path, \"w\") as result_file:\n",
        "        for query in queries:\n",
        "            result_set = boolean_query(query, inverted_index, total_docs)\n",
        "            result_filenames = convert_doc_ids_to_filenames(result_set, doc_id_to_filename)\n",
        "            result_str = f\"Results for '{query}': {result_filenames}\\n\"\n",
        "            print(result_str)\n",
        "            result_file.write(result_str)"
      ],
      "metadata": {
        "id": "o60Vg-iVm2an"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main Function**\n",
        "Defining the folder path (for the uploaded files) and loading text files. Then building inverted index and term frequencies. Setting example queries and processing each query and display the results."
      ],
      "metadata": {
        "id": "A2CsUoViBK1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Define folder path (for the uploaded files)\n",
        "    folder_path = '/content/drive/MyDrive/Week2assignment(IRS)'\n",
        "\n",
        "    # Load text files\n",
        "    data, doc_id_to_filename = load_text_files(folder_path)\n",
        "\n",
        "    # Build inverted index and term frequencies\n",
        "    inverted_index, term_frequencies = build_inverted_index(data)\n",
        "\n",
        "    # Example queries\n",
        "    queries = [\n",
        "        \"Life AND Death\",\n",
        "        \"Ambition OR happiness\",\n",
        "        \"NOT human\"\n",
        "    ]\n",
        "\n",
        "    # Process each query and display the results\n",
        "    write_query_results(queries, inverted_index, doc_id_to_filename, len(data))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sagYKlWYhqRK",
        "outputId": "69a2827d-0644-440b-df8a-4be79f6dfd68"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for 'Life AND Death': ['sample4.txt']\n",
            "\n",
            "Results for 'Ambition OR happiness': ['sample5.txt', 'sample3.txt']\n",
            "\n",
            "Results for 'NOT human': ['sample1.txt', 'sample5.txt', 'sample4.txt', 'sample3.txt']\n",
            "\n"
          ]
        }
      ]
    }
  ]
}